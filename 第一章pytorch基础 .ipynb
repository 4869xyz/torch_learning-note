{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a5dbd2-54e7-40a2-8f48-28d61d58f04a",
   "metadata": {},
   "source": [
    "# 第一章 pytorch基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abe793a-81e4-48f4-87d3-be7e7a6545a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f53d40-6785-4921-8e95-d1edc1da851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.zeros(2,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d76c87-d06c-4433-aa03-fe017a4cbdc5",
   "metadata": {},
   "source": [
    "张量（Tensor）\r\n",
    "张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。\r\n",
    "\r\n",
    "张量可以视为一个多维数组，支持加速计算的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e834ede-be55-41b4-8a5e-141a568a7cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5809,  0.1983, -0.2143],\n",
      "        [ 2.3237, -0.1553,  0.6574]])\n"
     ]
    }
   ],
   "source": [
    "b=torch.randn(2,3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a97feaf-4579-4410-ad45-398ab21416ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_array=np.array([[1,2],[3,4]])\n",
    "print(np_array)\n",
    "tensor_from_numpy=torch.from_numpy(np_array)\n",
    "print(tensor_from_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16e26e3-d4fe-4aee-a8bd-078e3e0dcc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7469,  0.1215,  0.5507],\n",
      "        [ 0.3500, -0.0789, -0.6234]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d=torch.randn(2,3,device=device)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a30822-7b91-4658-a59b-519c5a3422fa",
   "metadata": {},
   "source": [
    "常用的张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222fbdd5-f7c9-4790-8ee6-36c72b0159c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3987,  1.1449,  1.0324],\n",
      "        [ 0.5543, -1.9386, -1.1582]])\n"
     ]
    }
   ],
   "source": [
    "#相加\n",
    "# 张量相加\n",
    "e = torch.randn(2, 3)\n",
    "f = torch.randn(2, 3)\n",
    "print(e + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f04eb0a-94ce-470c-b320-99cfd06729f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e is  tensor([[-0.6197,  0.1066,  0.7047],\n",
      "        [-1.0959, -0.1800, -0.0347]])\n",
      "f is  tensor([[-0.7790,  1.0384,  0.3276],\n",
      "        [ 1.6501, -1.7586, -1.1234]])\n",
      "tensor([[ 0.4827,  0.1107,  0.2309],\n",
      "        [-1.8083,  0.3165,  0.0390]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#逐元素乘法，就是对应位置相乘\n",
    "print(\"e is \",e)\n",
    "print(\"f is \",f)\n",
    "print(e*f)\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddf34e1-6738-4ae6-a2f4-3b0d0824b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6197, -1.0959],\n",
      "        [ 0.1066, -0.1800],\n",
      "        [ 0.7047, -0.0347]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(e.t())## 张量的转置\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc06a3-84a7-4ac0-9e90-a89e1757971e",
   "metadata": {},
   "source": [
    "张量与设备\r\n",
    "PyTorch 张量可以存在于不同的设备上，包括CPU和GPU，你可以将张量移动到 GPU 上以加速计算：\r\n",
    "\r\n",
    "if torch.cuda.is_available():\r\n",
    "    tensor_gpu = tensor_from_list.to('cuda')  # 将张量移动到GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808797b6-0d41-4c28-837d-e9b2b73b856e",
   "metadata": {},
   "source": [
    "梯度和自动微分\n",
    "\n",
    "\n",
    "PyTorch的张量支持自动微分，这是深度学习中的关键特性。当你创建一个需要梯度的张量时，PyTorch可以自动计算其梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26573dcd-55ec-468f-a461-d71fe44e985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "tensor_requires_grad=torch.tensor([1.0],requires_grad=True)\n",
    "\n",
    "tensor_result=tensor_requires_grad*3\n",
    "\n",
    "tensor_result.backward()\n",
    "print(tensor_requires_grad.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2453e3a-92d7-4b79-a0b2-a64820312971",
   "metadata": {},
   "source": [
    "torch.Tensor 对象有一个 requires_grad 属性，用于指示是否需要计算该张量的梯度。\r\n",
    "\r\n",
    "当你创建一个 requires_grad=True 的张量时，PyTorch 会自动跟踪所有对它的操作，以便在之后计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74003ea2-b696-445e-8732-27bc54141b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2346, -2.3676],\n",
      "        [-0.4216, -1.3358]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(2,2,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa2edcdb-6fb9-4e88-a039-4c1e5396ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2346, -0.3676],\n",
      "        [ 1.5784,  0.6642]], grad_fn=<AddBackward0>)\n",
      "tensor([[14.9808,  0.4053],\n",
      "        [ 7.4738,  1.3234]], grad_fn=<MulBackward0>)\n",
      "tensor(6.0458, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y=x+2\n",
    "z=y*y*3\n",
    "out=z.mean()\n",
    "print(y)\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46269326-8e21-468f-920a-5a24695698eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.7039, -1.1027],\n",
      "        [ 4.7351,  1.9925]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09259a0-3f25-4c51-92fa-f9cb9bbff33e",
   "metadata": {},
   "source": [
    "因为反向传播只能对叶子节点进行啥的，然后y和z是中间变量，然后就不能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f897ffef-0373-4811-b0c0-2a0928256e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32mD:\\project_anaconda\\envs\\pytorch2\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\project_anaconda\\envs\\pytorch2\\lib\\site-packages\\torch\\autograd\\__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    252\u001b[0m     (inputs,)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    257\u001b[0m )\n\u001b[0;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mD:\\project_anaconda\\envs\\pytorch2\\lib\\site-packages\\torch\\autograd\\__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc0506-2ae9-4780-9c6f-0c33c3973807",
   "metadata": {},
   "source": [
    "停止梯度计算\r\n",
    "如果你不希望某些张量的梯度被计算（例如，当你不需要反向传播时），可以使用 torch.no_grad() 或设置 requires_grad=False。\r\n",
    "\r\n",
    " x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82e6ed72-7ecb-4619-97a0-45a9aa5aa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 torch.no_grad() 禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    y = x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9facea88-60b8-4755-be35-af86256240f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的全连接神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "151ad25a-4584-4781-ba82-22e99aca058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  tensor([[ 0.9095, -0.5487]])\n",
      "output  tensor([[0.5682]], grad_fn=<AddmmBackward0>)\n",
      "target tensor([[1.0276]])\n",
      "loss  tensor(0.2111, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 随机输入\n",
    "x=torch.randn(1,2)\n",
    "print(\"x \",x)\n",
    "# 前向传播\n",
    "output=model(x)\n",
    "print(\"output \",output)\n",
    "# 定义损失函数（例如均方误差 MSE）\n",
    "criterion=nn.MSELoss()\n",
    "# 假设目标值为 1\n",
    "target=torch.randn(1,1)\n",
    "print(\"target\",target)\n",
    "# 计算损失\n",
    "loss=criterion(output,target)\n",
    "print(\"loss \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24b158d2-eb3d-4771-9467-90a2437b7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器（使用 Adam 优化器）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练步骤\n",
    "optimizer.zero_grad()  # 清空梯度\n",
    "loss.backward()  # 反向传播\n",
    "optimizer.step()  # 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a07dec-33ea-4a00-861c-e347fd2ddc55",
   "metadata": {},
   "source": [
    "训练模型\r\n",
    "训练模型是机器学习和深度学习中的核心过程，旨在通过大量数据学习模型参数，以便模型能够对新的、未见过的数据做出准确的预测。\r\n",
    "\r\n",
    "训练模型通常包括以下几个步\n",
    "骤：\r\n",
    "\r\n",
    "数\n",
    "据准备：\r\n",
    "\r\n",
    "收集和处理数据，包括清洗、标准化和归一化。\r\n",
    "将数据分为训练集、\n",
    "\n",
    "验证集和测试集。\r\n",
    "定义模型：\r\n",
    "\r\n",
    "选择模型架构，例如决策树、神经网络等。\r\n",
    "初始化\n",
    "\n",
    "模型参数（权重和偏\n",
    "置）。\r\n",
    "选择损失函数：\r\n",
    "\r\n",
    "根据任务类型（如分\n",
    "\n",
    "类、回归）选择合\n",
    "适的损失函数。\r\n",
    "选择优化器：\r\n",
    "\r\n",
    "选择一个优化算法，如\n",
    "\n",
    "SGD、Ada\n",
    "m等，来更新模型参数。\r\n",
    "前向传播：\r\n",
    "\r\n",
    "在每次迭代\n",
    "\n",
    "中，将输入数据\n",
    "通过模型传递，计算预测输出。\r\n",
    "计算损失：\r\n",
    "\r\n",
    "\n",
    "\n",
    "使用损失函数\n",
    "评估预测输出与真实标签之间的差异。\r\n",
    "反向传\n",
    "\n",
    "播：\r\n",
    "\r\n",
    "利\n",
    "\n",
    "用自动求导计算损失相对于模型参数的梯度。\r\n",
    "参\n",
    "\n",
    "数更新：\r\n",
    "\r\n",
    "\n",
    "\n",
    "根据计算出的梯度和优化器的策略更新模型参数。\r\n",
    "迭代优化：\r\n",
    "\r\n",
    "重复\n",
    "\n",
    "步骤5-8，直到\n",
    "\n",
    "模型在验证集上的性能不再提升或达到预定的迭代次数。\r\n",
    "\n",
    "\n",
    "评估和测试：\n",
    "\r\n",
    "\r\n",
    "使用测试集评估模型的最终性能，确保模型没有过拟合。\r\n",
    "模\n",
    "\n",
    "型调优：\r\n",
    "\r\n",
    "根据模型在测试集上的表现进行调参，如改变学习率、增加正则化等。\r\n",
    "部署模型：\r\n",
    "\r\n",
    "将训练好的模型部署到生产环境中，用于实际的预测任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4452917-f9a0-4dbd-980d-5d3b46c734ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0289\n",
      "Epoch [20/100], Loss: 1.0174\n",
      "Epoch [30/100], Loss: 1.0065\n",
      "Epoch [40/100], Loss: 0.9961\n",
      "Epoch [50/100], Loss: 0.9878\n",
      "Epoch [60/100], Loss: 0.9800\n",
      "Epoch [70/100], Loss: 0.9727\n",
      "Epoch [80/100], Loss: 0.9660\n",
      "Epoch [90/100], Loss: 0.9596\n",
      "Epoch [100/100], Loss: 0.9536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器\n",
    "\n",
    "# 4. 假设我们有训练数据 X 和 Y\n",
    "X = torch.randn(10, 2)  # 10 个样本，2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标值\n",
    "\n",
    "# 5. 训练循环\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    \n",
    "    # 每 10 轮输出一次损失\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7658fa4-abf4-4707-828f-284716a7a426",
   "metadata": {},
   "source": [
    "设备device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65957dcd-80a6-4e1a-acd8-30e0d14217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 将模型移动到设备\n",
    "model.to(device)\n",
    "\n",
    "# 将数据移动到设备\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77867857-72be-4a19-a5f9-858487f7a061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
