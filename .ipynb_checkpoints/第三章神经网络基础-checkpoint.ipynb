{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddbbebd-9174-46bb-b0fa-80998c95032b",
   "metadata": {},
   "source": [
    "# 第三章神经网络基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8743d-51b5-4116-91f9-9b77c65e90c7",
   "metadata": {},
   "source": [
    "神经元（Neuron）\r\n",
    "神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，然后通过激活函数处理以产生输出。\r\n",
    "\r\n",
    "神经元的权重和偏置是网络学习过程中需要调整的参数。\r\n",
    "\r\n",
    "输入和输出:\r\n",
    "\r\n",
    "输入（Input）：输入是网络的起始点，可以是特征数据，如图像的像素值或文本的词向量。\r\n",
    "输出（Output）：输出是网络的终点，表示模型的预测结果，如分类任务中的类别标签。\r\n",
    "神经元接收多个输入（例如x1, x2, ..., xn），如果输入的加权和大于激活阈值（activation potential），\n",
    "\n",
    "则产生二进制输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152448cf-5d49-4327-aad4-0520fbf42bd4",
   "metadata": {},
   "source": [
    "在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类。\r\n",
    "\r\n",
    "nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：\r\n",
    "\r\n",
    "__i nit__()：定义网络层。\r\n",
    "forward()：定义数据的前向传播过程。\r\n",
    "简单的全连接神经网络（Fully Connected Network）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75fd4f-9a10-44d4-83f5-847cff88d54d",
   "metadata": {},
   "source": [
    "PyTorch 提供了许多常见的神经网络层，以下是几个常见的：\r\n",
    "\r\n",
    "nn.Linear(in_features, out_features)：全连接层，输入 in_features 个特征，输出 out_features 个特\n",
    "\n",
    "\n",
    "征。\r\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size)：2D 卷积层，用于图像\n",
    "\n",
    "\n",
    "\n",
    "处理。\r\n",
    "nn.MaxPool2d(kernel_size)：2D 最大池化层，用\n",
    "\n",
    "\n",
    "\n",
    "于降维。\r\n",
    "nn.ReLU()：ReLU 激活函数，常用\n",
    "\n",
    "\n",
    "\n",
    "于隐藏层。\r\n",
    "nn.Softmax(dim)：Softmax 激活函数，通常用于输出层，适用于多类分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f9eaedc-3b30-4a9d-89d5-86006a439421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 定义一个输入层到隐藏层的全连接层\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入 2 个特征，输出 2 个特征\n",
    "        # 定义一个隐藏层到输出层的全连接层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 输入 2 个特征，输出 1 个预测值\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 前向传播过程\n",
    "        x = torch.relu(self.fc1(x))  # 使用 ReLU 激活函数\n",
    "        x = self.fc2(x)  # 输出层\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a3489b0-bee6-4430-910d-399085d0da40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ReLU 激活\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43minput_tensor\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Sigmoid 激活\u001b[39;00m\n\u001b[0;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(input_tensor)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ReLU 激活\n",
    "output = F.relu(input_tensor)\n",
    "\n",
    "# Sigmoid 激活\n",
    "output = torch.sigmoid(input_tensor)\n",
    "\n",
    "# Tanh 激活\n",
    "output = torch.tanh(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a14885-297a-4459-9c60-5d75f08f6845",
   "metadata": {},
   "source": [
    "损失函数（Loss Function）\r\n",
    "损失函数用于衡量模型的预测值与真实值之间的差异。\r\n",
    "\r\n",
    "常见的损失函数包括：\r\n",
    "\r\n",
    "均方误差（MSELoss）：回归问题常用，计算输出与目标值的平方差。\r\n",
    "交叉熵损失（CrossEntropyLoss）：分类问题常用，计算输出和真实标签之间的交叉熵。\r\n",
    "BCEWithLogitsLoss：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58629838-94e5-4c6b-b3de-74e4331dbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 均方误差损失\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 二分类交叉熵损失\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b37c9040-a70b-4c25-804e-16e84fcdb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 使用 SGD 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 使用 Adam 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a021480-200c-46e4-b7a8-a9b0241f7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设已经定义好了模型、损失函数和优化器\n",
    "\n",
    "# 训练数据示例\n",
    "X = torch.randn(10, 2)  # 10 个样本，每个样本有 2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标标签\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    optimizer.zero_grad()  # 清除梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新权重\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:  # 每 10 轮输出一次损失\n",
    "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
